diff --git a/aa228/sb3_roundabout.py b/aa228/sb3_roundabout.py
index 1130365..66f5f6a 100644
--- a/aa228/sb3_roundabout.py
+++ b/aa228/sb3_roundabout.py
@@ -2,6 +2,9 @@ import gymnasium as gym
 from gymnasium.wrappers import RecordVideo
 from stable_baselines3 import DQN
 from stable_baselines3 import PPO
+# from stable_baselines3.common.callbacks import WandbCallback # <-- New Import
+from wandb.integration.sb3 import WandbCallback
+import wandb # <-- New Import
 
 # import roundabout_env  # noqa: F401
 from highway_env.envs.aa228_env import AA228Env
@@ -57,9 +60,30 @@ if __name__ == "__main__":
 
     # Train the model
     if TRAIN:
-        model.learn(total_timesteps=int(1e2))
-        # model.learn(total_timesteps=int(2e4))
+        # Initialize wandb run
+        run = wandb.init(
+            project="aa228-highway-env",  # Replace with your desired project name
+            sync_tensorboard=True,        # Sync SB3's TensorBoard logs
+            monitor_gym=True,             # Monitor gym/gymnasium environments
+            save_code=True                # Save the script file
+        )
+
+        # Define the WandbCallback
+        wandb_callback = WandbCallback(
+            model_save_path="roundabout_" + model_str + "/wandb_models", # Folder to save models to
+            verbose=2,
+        )
+
+        # Pass the callback to model.learn()
+        model.learn(total_timesteps=int(2e2), callback=wandb_callback)
+        # model.learn(total_timesteps=int(2e4), callback=wandb_callback)
+        
+        # Save the final model outside of the callback
         model.save("roundabout_" + model_str + "/model")
+        
+        # Finish the wandb run
+        run.finish()
+        
         del model
 
     # Run the trained model and record video
@@ -71,9 +95,9 @@ if __name__ == "__main__":
         env, video_folder="roundabout_" + model_str + "/videos", episode_trigger=lambda e: True
     )
     env.unwrapped.config["simulation_frequency"] = 15  # Higher FPS for rendering
-    # env.unwrapped.set_record_video_wrapper(env)
+    env.unwrapped.set_record_video_wrapper(env)
 
-    for videos in range(3):
+    for videos in range(10):
         done = truncated = False
         obs, info = env.reset()
         while not (done or truncated):
@@ -83,4 +107,4 @@ if __name__ == "__main__":
             obs, reward, done, truncated, info = env.step(action)
             # Render
             env.render()
-    # env.close()
+    env.close()
\ No newline at end of file
diff --git a/highway_env/envs/aa228_env.py b/highway_env/envs/aa228_env.py
index 2f600ca..193495a 100644
--- a/highway_env/envs/aa228_env.py
+++ b/highway_env/envs/aa228_env.py
@@ -16,28 +16,42 @@ class AA228Env(AbstractEnv):
         config.update(
             {
                 "observation": {
+                    # "type": "OccupancyGrid",
+                    # "vehicles_count": 15,
+                    # "features": ["presence", "x", "y", "vx", "vy", "cos_h", "sin_h"],
+                    # "features_range": {
+                    #     "x": [-100, 100],
+                    #     "y": [-100, 100],
+                    #     "vx": [-20, 20],
+                    #     "vy": [-20, 20]
+                    # },
+                    # "grid_size": [[-27.5, 27.5], [-27.5, 27.5]],
+                    # "grid_step": [5, 5],
+                    # "absolute": False
                     "type": "Kinematics",
-                    "absolute": True,
                     "vehicles_count": 10,
+                    "features": ["presence", "x", "y", "vx", "vy", "cos_h", "sin_h"],
                     "features_range": {
                         "x": [-100, 100],
                         "y": [-100, 100],
-                        "vx": [-15, 15],
-                        "vy": [-15, 15],
+                        "vx": [-20, 20],
+                        "vy": [-20, 20]
                     },
+                    "absolute": True,
+                    "order": "sorted",
                 },
                 # "action": {"type": "ContinuousAction"}, # not for DQN
                 # "action": {"type": "DiscreteMetaAction", "target_speeds": [0, 8, 16]},
-                "action": {"type": "DiscreteMetaAction"},
+                "action": {"type": "DiscreteMetaAction", "target_speeds": [0, 4, 8, 12, 16]},
                 "incoming_vehicle_destination": None,
-                "collision_reward": -100000,
+                "collision_reward": -1.0,
                 "high_speed_reward": 0.2,
                 "right_lane_reward": 0,
                 # "lane_change_reward": -0.05,
                 "screen_width": 600,
                 "screen_height": 600,
                 "centering_position": [0.5, 0.6],
-                "duration": 11,
+                "duration": 20,
                 "normalize_reward": True,
             }
         )
@@ -356,7 +370,7 @@ class AA228Env(AbstractEnv):
             self.road,
             ("we", "sx", 0),
             longitudinal=5.0 + self.np_random.normal() * position_deviation,
-            speed=16 + self.np_random.normal() * speed_deviation,
+            speed=10 + self.np_random.normal() * speed_deviation,
         )
 
         if self.config["incoming_vehicle_destination"] is not None:
@@ -373,9 +387,9 @@ class AA228Env(AbstractEnv):
             vehicle = other_vehicles_type.make_on_lane(
                 self.road,
                 ("we", "sx", 0),
-                longitudinal=20.0 * float(i)
+                longitudinal= 40.0 * float(i)
                 + self.np_random.normal() * position_deviation,
-                speed=16.0 + self.np_random.normal() * speed_deviation,
+                speed=8.0 + self.np_random.normal() * speed_deviation,
             )
             vehicle.plan_route_to(self.np_random.choice(destinations))
             vehicle.randomize_behavior()
@@ -387,7 +401,7 @@ class AA228Env(AbstractEnv):
                 self.road,
                 ("eer", "ees", 0),
                 longitudinal= 20.0 * float(i) + self.np_random.normal() * position_deviation,
-                speed=16.0 + self.np_random.normal() * speed_deviation,
+                speed=5.0 + self.np_random.normal() * speed_deviation,
             )
             vehicle.plan_route_to(self.np_random.choice(destinations))
             vehicle.randomize_behavior()
diff --git a/highway_env/envs/roundabout_env.py b/highway_env/envs/roundabout_env.py
index 38c4f64..d3999e0 100644
--- a/highway_env/envs/roundabout_env.py
+++ b/highway_env/envs/roundabout_env.py
@@ -25,16 +25,16 @@ class RoundaboutEnv(AbstractEnv):
                         "vy": [-15, 15],
                     },
                 },
-                "action": {"type": "DiscreteMetaAction", "target_speeds": [0, 8, 16]},
+                "action": {"type": "DiscreteMetaAction", "target_speeds": [0, 2, 4, 6, 8, 10]},
                 "incoming_vehicle_destination": None,
                 "collision_reward": -1,
-                "high_speed_reward": 0.2,
+                "high_speed_reward": 1.0,
                 "right_lane_reward": 0,
                 "lane_change_reward": -0.05,
                 "screen_width": 600,
                 "screen_height": 600,
                 "centering_position": [0.5, 0.6],
-                "duration": 11,
+                "duration": 20,
                 "normalize_reward": True,
             }
         )
@@ -360,16 +360,16 @@ class RoundaboutEnv(AbstractEnv):
             destination = self.np_random.choice(destinations)
         vehicle.plan_route_to(destination)
         vehicle.randomize_behavior()
-        self.road.vehicles.append(vehicle)
+        # self.road.vehicles.append(vehicle)
 
         # Other vehicles
-        for i in list(range(1, 2)) + list(range(-1, 0)):
+        for i in list(range(10)):
             vehicle = other_vehicles_type.make_on_lane(
                 self.road,
                 ("we", "sx", 0),
                 longitudinal=20.0 * float(i)
                 + self.np_random.normal() * position_deviation,
-                speed=16.0 + self.np_random.normal() * speed_deviation,
+                speed=8.0 + self.np_random.normal() * speed_deviation,
             )
             vehicle.plan_route_to(self.np_random.choice(destinations))
             vehicle.randomize_behavior()
@@ -384,4 +384,4 @@ class RoundaboutEnv(AbstractEnv):
         )
         vehicle.plan_route_to(self.np_random.choice(destinations))
         vehicle.randomize_behavior()
-        self.road.vehicles.append(vehicle)
\ No newline at end of file
+        # self.road.vehicles.append(vehicle)
\ No newline at end of file
diff --git a/scripts/sb3_roundabout_dqn.py b/scripts/sb3_roundabout_dqn.py
index 7c8ce3a..d8b1ae7 100644
--- a/scripts/sb3_roundabout_dqn.py
+++ b/scripts/sb3_roundabout_dqn.py
@@ -1,11 +1,16 @@
 import gymnasium as gym
 from gymnasium.wrappers import RecordVideo
 from stable_baselines3 import DQN
+from stable_baselines3 import PPO
 
 # import roundabout_env  # noqa: F401
 from highway_env.envs.roundabout_env import RoundaboutEnv
 
 
+from wandb.integration.sb3 import WandbCallback
+import wandb # <-- New Import
+
+
 TRAIN = True
 
 if __name__ == "__main__":
@@ -14,30 +19,62 @@ if __name__ == "__main__":
     obs, info = env.reset()
 
     # Create the model
-    model = DQN(
+    n_cpu = 6
+    batch_size = 32
+    model = PPO(
         "MlpPolicy",
         env,
-        policy_kwargs=dict(net_arch=[256, 256]),
+        policy_kwargs=dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])]),
+        n_steps=batch_size * 12 // n_cpu,
+        batch_size=batch_size,
+        n_epochs=10,
         learning_rate=5e-4,
-        buffer_size=15000,
-        learning_starts=200,
-        batch_size=32,
-        gamma=0.8,
-        train_freq=1,
-        gradient_steps=1,
-        target_update_interval=50,
-        verbose=1,
-        tensorboard_log="roundabout_dqn/",
+        gamma=0.9,
+        verbose=2,
+        tensorboard_log="roundabout_ppo/",
     )
+    # # Create the model
+    # model = DQN(
+    #     "MlpPolicy",
+    #     env,
+    #     policy_kwargs=dict(net_arch=[256, 256]),
+    #     learning_rate=5e-4,
+    #     buffer_size=15000,
+    #     learning_starts=200,
+    #     batch_size=32,
+    #     gamma=0.8,
+    #     train_freq=1,
+    #     gradient_steps=1,
+    #     target_update_interval=50,
+    #     verbose=1,
+    #     tensorboard_log="roundabout_dqn/",
+    # )
 
     # Train the model
     if TRAIN:
-        model.learn(total_timesteps=int(2e4))
+        run = wandb.init(
+            project="aa228-highway-env",  # Replace with your desired project name
+            sync_tensorboard=True,        # Sync SB3's TensorBoard logs
+            monitor_gym=True,             # Monitor gym/gymnasium environments
+            save_code=True                # Save the script file
+        )
+
+        # Define the WandbCallback
+        wandb_callback = WandbCallback(
+            model_save_path="roundabout_ppo/wandb_models", # Folder to save models to
+            verbose=2,
+        )
+
+        # Pass the callback to model.learn()
+        model.learn(total_timesteps=int(2e2), callback=wandb_callback)
+    
         model.save("roundabout_dqn/model")
+        run.finish()
+
         del model
 
     # Run the trained model and record video
-    model = DQN.load("roundabout_dqn/model", env=env)
+    model = PPO.load("roundabout_dqn/model", env=env)
     env = RecordVideo(
         env, video_folder="roundabout_dqn/videos", episode_trigger=lambda e: True
     )
